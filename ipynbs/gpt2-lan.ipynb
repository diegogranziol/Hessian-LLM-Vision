{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa20383e-7915-47f5-afd5-2e6b28fcdf45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision==0.16.2\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.16.2) (1.26.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.16.2) (2.32.3)\n",
      "Requirement already satisfied: torch==2.1.2 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.16.2) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.16.2) (10.2.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->torchvision==0.16.2) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->torchvision==0.16.2) (12.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.16.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.16.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.16.2) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.16.2) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.2->torchvision==0.16.2) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.2->torchvision==0.16.2) (1.3.0)\n",
      "Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.1\n",
      "    Uninstalling torchvision-0.17.1:\n",
      "      Successfully uninstalled torchvision-0.17.1\n",
      "Successfully installed torchvision-0.16.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision==0.16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e9d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Config\n",
    "from torch.optim import SGD\n",
    "import torch\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import gpytorch\n",
    "import gc\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b84a358e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = \"distilgpt2\"  # Using a smaller model for demonstration\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7ea9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
    "\n",
    "subsample_size = int(0.001 * len(ds['train']))\n",
    "subsample = ds['train'].shuffle(seed=42).select(range(subsample_size))\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "tokenized_docs = subsample.map(tokenize_function, batched=True)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def select_model_inputs(batch):\n",
    "    return {\n",
    "        \"input_ids\": batch[\"input_ids\"],\n",
    "        \"attention_mask\": batch[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "model_inputs = tokenized_docs.map(select_model_inputs, batched=True)\n",
    "\n",
    "# Manually collate a batch\n",
    "def manual_collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    return {\n",
    "        'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# Create DataLoader\n",
    "data_loader = DataLoader(model_inputs, batch_size=1, collate_fn=manual_collate_fn)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d8e30f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurvVecProduct:\n",
    "    def __init__(self, inputs, model, criterion, labels, init_vec=None):\n",
    "        self.inputs = inputs\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.labels = labels\n",
    "        self.init_vec = init_vec\n",
    "        self.iters = 0\n",
    "\n",
    "    def __call__(self, vector):\n",
    "        if self.iters == 0 and self.init_vec is not None:\n",
    "            vector = self.init_vec\n",
    "        output = hess_vec(vector, self.inputs, self.model, self.criterion, self.labels, cuda=device.type == 'cuda')\n",
    "        self.iters += 1\n",
    "        return output.unsqueeze(1)\n",
    "\n",
    "# Define the Hessian-vector product function\n",
    "def hess_vec(vector, inputs, model, criterion, labels, cuda=True):\n",
    "    param_list = list(model.parameters())\n",
    "    vector_list = []\n",
    "\n",
    "    offset = 0\n",
    "    for param in param_list:\n",
    "        vector_list.append(vector[offset:offset + param.numel()].detach().view_as(param).to(param.device))\n",
    "        offset += param.numel()\n",
    "\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "    outputs = model(inputs, labels=inputs)\n",
    "    loss = outputs.loss\n",
    "    # loss = criterion(outputs, labels)\n",
    "    #loss = loss.mean()\n",
    "\n",
    "    grad_list = torch.autograd.grad(loss, param_list, create_graph=True)\n",
    "    dL_dvec = torch.zeros(1, device='cuda' if cuda else 'cpu')\n",
    "    for v, g in zip(vector_list, grad_list):\n",
    "        dL_dvec += torch.sum(v * g)\n",
    "    dL_dvec.backward()\n",
    "\n",
    "    return torch.cat([param.grad.view(-1) for param in param_list]).view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cca5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before curvvec\n",
      "after curvvec\n",
      "Epoch: 1, Loss: 3.002678394317627\n",
      "before curvvec\n",
      "after curvvec\n",
      "Epoch: 1, Loss: 8.563395500183105\n",
      "before curvvec\n",
      "after curvvec\n",
      "Epoch: 1, Loss: 2.373828887939453\n",
      "before curvvec\n",
      "after curvvec\n",
      "Epoch: 1, Loss: 2.3428092002868652\n",
      "before curvvec\n",
      "after curvvec\n",
      "Epoch: 1, Loss: 3.6841518878936768\n",
      "before curvvec\n",
      "after curvvec\n",
      "Epoch: 1, Loss: 8.902929306030273\n",
      "before curvvec\n",
      "after curvvec\n",
      "Epoch: 1, Loss: 3.975749969482422\n",
      "before curvvec\n",
      "after curvvec\n",
      "Epoch: 1, Loss: 3.765594720840454\n",
      "before curvvec\n",
      "after curvvec\n",
      "Epoch: 1, Loss: 5.644918918609619\n",
      "before curvvec\n"
     ]
    }
   ],
   "source": [
    "# Set the model to training mode\n",
    "momentum_buffers = {}\n",
    "\n",
    "model.train()\n",
    "losses = []\n",
    "# Check if MPS is available\n",
    "learning_rate = 1e-2\n",
    "lanczos_iters = 20\n",
    "delta = 1e-3\n",
    "momentum = 0\n",
    "weight_decay = 0\n",
    "smoothing = 0.7\n",
    "regularity = 1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 1\n",
    "\n",
    "# Training loop\n",
    "for batch_idx, batch in enumerate(data_loader):\n",
    "    # Prepare inputs and labels\n",
    "    inputs = batch[\"input_ids\"].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs, labels=inputs)\n",
    "    loss = outputs.loss\n",
    "    gradients = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "    grad_vector = torch.cat([grad.view(-1) for grad in gradients])\n",
    "\n",
    "    if (batch_idx+1) % regularity == 0 or batch_idx == 0:\n",
    "\n",
    "        # Curvature vector product and Lanczos tridiagonalization\n",
    "        print(\"before curvvec\")\n",
    "        productor = CurvVecProduct(inputs, model, criterion, inputs, init_vec=grad_vector)\n",
    "        P = sum(p.numel() for p in model.parameters())\n",
    "        Q, T = gpytorch.utils.lanczos.lanczos_tridiag(\n",
    "            productor,\n",
    "            max_iter=lanczos_iters,\n",
    "            dtype=torch.float32,\n",
    "            device=device,\n",
    "            matrix_shape=(P, P)\n",
    "        )\n",
    "        print(\"after curvvec\")\n",
    "        # Compute eigenvalues and eigenvectors\n",
    "        c_eigvals, c_eigvects = torch.linalg.eigh(T)\n",
    "        c_gammas = c_eigvects[0, :] ** 2\n",
    "        c_V = c_eigvects.t() @ Q.t()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            eigvals, V = c_eigvals[:], c_V[:]\n",
    "        elif not torch.isnan(c_eigvals).any():\n",
    "            eigvals = smoothing*eigvals + (1-smoothing)*c_eigvals[:]\n",
    "            V = smoothing*V+(1-smoothing)*c_V[:]\n",
    "        else:\n",
    "            print(\"failure to calculate curvature\")\n",
    "\n",
    "    adjusted_grad_vector = grad_vector.clone()\n",
    "    new_grad = torch.zeros_like(grad_vector).to(device)\n",
    "    \n",
    "    for i, eigval in enumerate(eigvals):\n",
    "        intermediate_vec = V[i].to(device)\n",
    "        dot_product = torch.dot(grad_vector, intermediate_vec)\n",
    "        adjustment = (1 / (abs(eigval) + delta)) * dot_product * intermediate_vec\n",
    "        new_grad += adjustment\n",
    "    \n",
    "    split_sizes = [p.numel() for p in model.parameters()]\n",
    "    split_gradients = torch.split(new_grad, split_sizes)\n",
    "    adjusted_gradients = [g.view(p.size()) for g, p in zip(split_gradients, model.parameters())]\n",
    "\n",
    "    # Update parameters with momentum and weight decay\n",
    "    with torch.no_grad():\n",
    "        for param, adj_grad in zip(model.parameters(), adjusted_gradients):\n",
    "            # Calculate weight decay term if applicable\n",
    "            weight_decay_term = weight_decay * param.data if weight_decay != 0 else 0\n",
    "            adjusted_grad_with_weight_decay = adj_grad + weight_decay_term\n",
    "\n",
    "            # Update momentum buffer\n",
    "            if param in momentum_buffers:\n",
    "                momentum_buffers[param] = momentum_buffers[param] * momentum + adjusted_grad_with_weight_decay\n",
    "            else:\n",
    "                momentum_buffers[param] = adjusted_grad_with_weight_decay\n",
    "\n",
    "            # Apply the update to parameters\n",
    "            param.data -= learning_rate * momentum_buffers[param]\n",
    "\n",
    "            # Optionally set param.grad for potential further gradient manipulations\n",
    "            param.grad = momentum_buffers[param]\n",
    "\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    print(f\"Epoch: {1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5798f8-0ebe-4ad9-b309-93eab8e741be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
